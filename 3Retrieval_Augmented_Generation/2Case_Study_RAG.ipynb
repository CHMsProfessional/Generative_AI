{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Retrieval Augmented Generation\n",
    "### Case Study: RAG Pipeline\n",
    "### LangChain + Azure OpenAI + Pinecone\n",
    "<span>Â© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) for question answering using PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Note: This notebook requires Python 3.11. You can download from here https://www.python.org/ftp/python/3.11.0/python-3.11.0rc2-amd64.exe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Dependencies \n",
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read Pinecone and Azure OpenAI Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read Pinecone and Azure OpenAI Environment Variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_AZURE_OPENAI_API_KEY\" #key from the Azure OpenAI resource\n",
    "os.environ[\"AZURE_OPENAI_API_BASE\"] = \"YOUR_AZURE_OPENAI_API_BASE\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\" #key from the Pinecone resource\n",
    "\n",
    "openai.api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load your PDF and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23 document(s) and split into 78 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load your PDF and split into chunks\n",
    "pdf_path = \"./docs/corollacross_brochure.pdf\"  # Adjust the file path if needed\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} document(s) and split into {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Azure OpenAI embeddings object using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the Azure OpenAI embeddings object using LangChain.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=openai.api_key,\n",
    "    azure_endpoint=openai.api_base,  \n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Connect to Pinecone Client and get all availible indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indexes: ['langchain-demo2', 'langchain-demo', 'assignment4']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace these values as needed\n",
    "api_key = \"YOUR_PINECONE_API_KEY\"\n",
    "\n",
    "# Create an instance of the Pinecone class using the new API\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# List indexes to check connectivity\n",
    "print(\"Available indexes:\", pc.list_indexes().names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create index if you havent, it doesnt create if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-demo2\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536, # The text-embedding-ada-0002 model has 1536 dimensions\n",
    "    metric=\"cosine\", # Cosine is the it's one of the most common distance metrics used with text embeddings like text-embedding-ada-0002.\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 Create and store embeddings using the PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been successfully stored in Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Create and store embeddings using the PineconeVectorStore\n",
    "\n",
    "index_name = \"langchain-demo2\"\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name, \n",
    "    embedding=embeddings\n",
    "    )\n",
    "\n",
    "# Assuming 'docs' contains your document chunks\n",
    "vectorstore.add_documents(docs)\n",
    "\n",
    "print(\"Embeddings have been successfully stored in Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Perform a similarity search and retrieve the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_14408\\663328907.py:4: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://khipus-aoai.openai.azure.com to https://khipus-aoai.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://khipus-aoai.openai.azure.com to https://khipus-aoai.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_14408\\663328907.py:13: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Perform a similarity search and retrieve the most relevant documents\n",
    "\n",
    "# Initialize the language model using Azure Chat OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_base=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\"),\n",
    "    deployment_name=os.environ.get(\"AZURE_OPENAI_GPT4_MODEL_NAME\", \"gpt-4o\")\n",
    ")\n",
    "\n",
    "# Load the QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'creationdate': '2024-01-17T14:56:22-05:00', 'creator': 'Adobe InDesign 18.5 (Macintosh)', 'moddate': '2024-01-17T14:57:41-05:00', 'page': 2.0, 'page_label': '3', 'producer': 'Adobe PDF Library 17.0', 'source': './corollacross_brochure.pdf', 'total_pages': 23.0, 'trapped': '/False'}\n",
      "Metadata: {'creationdate': '2024-01-17T14:56:22-05:00', 'creator': 'Adobe InDesign 18.5 (Macintosh)', 'moddate': '2024-01-17T14:57:41-05:00', 'page': 2.0, 'page_label': '3', 'producer': 'Adobe PDF Library 17.0', 'source': './docs/corollacross_brochure.pdf', 'total_pages': 23.0, 'trapped': '/False'}\n",
      "Metadata: {'creationdate': '2024-01-17T14:56:22-05:00', 'creator': 'Adobe InDesign 18.5 (Macintosh)', 'moddate': '2024-01-17T14:57:41-05:00', 'page': 1.0, 'page_label': '2', 'producer': 'Adobe PDF Library 17.0', 'source': './docs/corollacross_brochure.pdf', 'total_pages': 23.0, 'trapped': '/False'}\n",
      "Metadata: {'creationdate': '2024-01-17T14:56:22-05:00', 'creator': 'Adobe InDesign 18.5 (Macintosh)', 'moddate': '2024-01-17T14:57:41-05:00', 'page': 1.0, 'page_label': '2', 'producer': 'Adobe PDF Library 17.0', 'source': './corollacross_brochure.pdf', 'total_pages': 23.0, 'trapped': '/False'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_14408\\550836211.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(input_documents=docs, question=query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "The Toyota Corolla Cross features a 2.0-liter engine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your query\n",
    "query = \"What is the engine size of the Toyota Corolla Cross?\"\n",
    "#What is the estimated fuel efficiency of the Corolla Cross Hybrid?\n",
    "\n",
    "# Retrieve similar documents from the vector store (removed include_metadata)\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "# Optionally, access metadata from the documents if needed\n",
    "for doc in docs:\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "\n",
    "\n",
    "\n",
    "# Get the answer from the chain\n",
    "result = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(f\"Answer: \\n\\n{result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
