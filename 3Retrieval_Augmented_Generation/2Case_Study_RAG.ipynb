{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Retrieval Augmented Generation\n",
    "### Case Study: RAG Pipeline\n",
    "### LangChain + Azure OpenAI + Pinecone\n",
    "<span>Â© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) for question answering using PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Note: This notebook requires Python 3.11. You can download from here https://www.python.org/ftp/python/3.11.0/python-3.11.0rc2-amd64.exe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Dependencies \n",
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read Pinecone and Azure OpenAI Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read Pinecone and Azure OpenAI Environment Variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_AZURE_OPENAI_API_KEY\" #key from the Azure OpenAI resource\n",
    "os.environ[\"AZURE_OPENAI_API_BASE\"] = \"YOUR_AZURE_OPENAI_ENDPOINT\"#https://azure-openai-<your-resource-name>.openai.azure.com/\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "openai.api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load your PDF and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load your PDF and split into chunks\n",
    "pdf_path = \"./docs/corollacross_brochure.pdf\"  # Adjust the file path if needed\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} document(s) and split into {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Azure OpenAI embeddings object using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the Azure OpenAI embeddings object using LangChain.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=openai.api_key,\n",
    "    azure_endpoint=openai.api_base,  \n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate embeddings for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate embeddings for each chunk\n",
    "pinecone.Index = pinecone.data.index.Index\n",
    "\n",
    "# Replace these values as needed\n",
    "api_key = \"YOUR_PINECONE_API_KEY\" \n",
    "index_name = \"langchain-demo\"\n",
    "\n",
    "# Create an instance of the Pinecone class using the new API\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# List indexes to check connectivity\n",
    "print(\"Available indexes:\", pc.list_indexes().names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Create and store embeddings using the PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 Create and store embeddings using the PineconeVectorStore\n",
    "# Retrieve the index client\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Create and store embeddings using the PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(\n",
    "    index,         # The instance of pinecone.Index\n",
    "    embeddings,    # Your initialized embeddings object (Azure OpenAI embeddings)\n",
    "    text_key=\"text\",  # Adjust if your documents use a different key\n",
    "    namespace=\"default\"\n",
    ")\n",
    "\n",
    "# Assuming 'docs' contains your document chunks\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "print(\"Embeddings have been successfully stored in Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Perform a similarity search and retrieve the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perform a similarity search and retrieve the most relevant documents\n",
    "\n",
    "# Initialize the language model using Azure Chat OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_base=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\"),\n",
    "    deployment_name=os.environ.get(\"AZURE_OPENAI_GPT4_MODEL_NAME\", \"gpt-4o\")\n",
    ")\n",
    "\n",
    "# Load the QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your query\n",
    "query = \"What is the engine size of the Toyota Corolla Cross?\"\n",
    "#What is the estimated fuel efficiency of the Corolla Cross Hybrid?\n",
    "\n",
    "# Retrieve similar documents from the vector store (removed include_metadata)\n",
    "docs = vector_store.similarity_search(query)\n",
    "\n",
    "# Optionally, access metadata from the documents if needed\n",
    "for doc in docs:\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "\n",
    "\n",
    "\n",
    "# Get the answer from the chain\n",
    "result = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(f\"Answer: \\n\\n{result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
