{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Retrieval Augmented Generation\n",
    "### RAG Assignment 4\n",
    "### LangChain + Azure OpenAI + Pinecone\n",
    "### Name: (You Name here)\n",
    "<span>© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Note: This notebook requires Python 3.11. You can download from here https://www.python.org/ftp/python/3.11.0/python-3.11.0rc2-amd64.exe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) for question answering using PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Dependencies \n",
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read Pinecone and Azure OpenAI Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read Pinecone and Azure OpenAI Environment Variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_AZURE_OPENAI_API_KEY\" #key from the Azure OpenAI resource\n",
    "os.environ[\"AZURE_OPENAI_API_BASE\"] = \"YOUR_AZURE_OPENAI_ENDPOINT\"#https://azure-openai-<your-resource-name>.openai.azure.com/\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "openai.api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load your PDF and split into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will load your employee_handbook.pdf file and split it into smaller chunks for processing. The document will be divided into manageable segments to create embeddings that can be stored in our vector database.\n",
    "\n",
    "To use your own employee handbook:\n",
    "1. Place your PDF file in the `./docs/` directory\n",
    "2. Update the `pdf_path` variable in the next cell to point to your file\n",
    "3. The handbook will be automatically chunked into segments of 1000 characters with 100 character overlap\n",
    "4. These chunks will later be embedded and stored in Pinecone for retrieval\n",
    "\n",
    "This approach allows you to query information from the employee handbook using natural language questions in the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Azure OpenAI embeddings object using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the Azure OpenAI embeddings object using LangChain.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=openai.api_key,\n",
    "    azure_endpoint=openai.api_base,  \n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate embeddings for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate embeddings for each chunk\n",
    "pinecone.Index = pinecone.data.index.Index\n",
    "\n",
    "# Replace these values as needed\n",
    "api_key = \"YOUR_PINECONE_API_KEY\" \n",
    "index_name = \"assignment4\"\n",
    "\n",
    "# Create an instance of the Pinecone class using the new API\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# List indexes to check connectivity\n",
    "print(\"Available indexes:\", pc.list_indexes().names())\n",
    "\n",
    "# Create (or connect to) your Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  \n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"  \n",
    "        )\n",
    "    )\n",
    "    print(f\"Created index: {index_name}\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Create and store embeddings using the PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 Create and store embeddings using the PineconeVectorStore\n",
    "# Retrieve the index client\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Create and store embeddings using the PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(\n",
    "    index,         # The instance of pinecone.Index\n",
    "    embeddings,    # Your initialized embeddings object (Azure OpenAI embeddings)\n",
    "    text_key=\"text\",  # Adjust if your documents use a different key\n",
    "    namespace=\"default\"\n",
    ")\n",
    "\n",
    "# Assuming 'docs' contains your document chunks\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "print(\"Embeddings have been successfully stored in Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Perform a similarity search and retrieve the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perform a similarity search and retrieve the most relevant documents\n",
    "\n",
    "# Initialize the language model using Azure Chat OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_base=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\"),\n",
    "    deployment_name=os.environ.get(\"AZURE_OPENAI_GPT4_MODEL_NAME\", \"gpt-4o\")\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Load the QA chain and run the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Load the QA chain and run the query\n",
    "# Load the QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Define your query\n",
    "query = \"What is Contoso Electronics’ mission statement?\"\n",
    "#How often are employee performance reviews conducted?\n",
    "\n",
    "# Retrieve similar documents from the vector store (removed include_metadata)\n",
    "docs = vector_store.similarity_search(query)\n",
    "\n",
    "# Get the answer from the chain\n",
    "result = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(f\"Answer: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Test Your RAG System\n",
    "\n",
    "Test your Retrieval Augmented Generation system using the following questions. For each question, replace the `query` variable in the previous cell and run it to see how well your system retrieves and answers based on the document content:\n",
    "\n",
    "1. How can employees report unethical or illegal conduct confidentially?\n",
    "2. How often are employee performance reviews conducted?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#  How can employees report unethical or illegal conduct confidentially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youe code here\n",
    "# How often are employee performance reviews conducted?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
