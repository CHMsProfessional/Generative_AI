{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "### Case Study: Evaluate with quantitative NLP evaluators using Azure AI Evaluation SDK\n",
    "### Azure OpenAI + Azure AI Evaluation SDK\n",
    "<span>Â© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook demonstrates how to use NLP-based evaluators to assess the quality of generated text by comparing it to reference text. By the end of this tutorial, you'll be able to:\n",
    " - Understand different NLP evaluators such as `BleuScoreEvaluator`, `GleuScoreEvaluator`, `MeteorScoreEvaluator`, and `RougeScoreEvaluator`.\n",
    " - Evaluate dataset using these evaluators.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "#%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"YOUR-SUBSCRIPTION-ID\",#<YOUR-SUBSCRIPTION-ID>\n",
    "    \"resource_group_name\": \"YOUR-AZURE-RESOURCE-GROU\",# khipus-ai-gpt\n",
    "    \"project_name\": \"YOUR-PROJECT-NAME\",# khipus-ai-gpt2025\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NLP Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BleuScoreEvaluator\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language processing (NLP) and machine\n",
    "translation. It is widely used in text summarization and text generation use cases. It evaluates how closely the\n",
    "generated text matches the reference text. The BLEU score ranges from 0 to 1, with higher scores indicating\n",
    "better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu = BleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_score': 0.22961813530951883, 'bleu_result': 'fail', 'bleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = bleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_score': 1.0, 'bleu_result': 'pass', 'bleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Make the candidate response and the reference identical (or nearly so)\n",
    "result = bleu(\n",
    "    response=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"Tokyo is the capital of Japan.\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GleuScoreEvaluator\n",
    "\n",
    "The GLEU (Google-BLEU) score evaluator measures the similarity between generated and reference texts by\n",
    "evaluating n-gram overlap, considering both precision and recall. This balanced evaluation, designed for\n",
    "sentence-level assessment, makes it ideal for detailed analysis of translation quality. GLEU is well-suited for\n",
    "use cases such as machine translation, text summarization, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "gleu = GleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gleu_score': 0.4090909090909091, 'gleu_result': 'fail', 'gleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = gleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeteorScoreEvaluator\n",
    "\n",
    "The METEOR (Metric for Evaluation of Translation with Explicit Ordering) score grader evaluates generated text by\n",
    "comparing it to reference texts, focusing on precision, recall, and content alignment. It addresses limitations of\n",
    "other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR score considers synonyms and\n",
    "word stems to more accurately capture meaning and language variations. In addition to machine translation and\n",
    "text summarization, paraphrase detection is an optimal use case for the METEOR score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor = MeteorScoreEvaluator(alpha=0.9, beta=3.0, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor_score': 0.9067055393586005, 'meteor_result': 'fail', 'meteor_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = meteor(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RougeScoreEvaluator\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic\n",
    "summarization and machine translation. It measures the overlap between generated text and reference summaries.\n",
    "ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. Text\n",
    "summarization and document comparison are among optimal use cases for ROUGE, particularly in scenarios where text\n",
    "coherence and relevance are critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge_precision': 1.0, 'rouge_recall': 1.0, 'rouge_f1_score': 1.0, 'rouge_precision_result': 'pass', 'rouge_recall_result': 'pass', 'rouge_f1_score_result': 'pass', 'rouge_precision_threshold': 0.5, 'rouge_recall_threshold': 0.5, 'rouge_f1_score_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = rouge(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
