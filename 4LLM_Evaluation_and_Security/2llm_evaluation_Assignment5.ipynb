{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Khipus.ai\n",
                "### Assignment 5: Qualitative Metrics for LLM Evaluation Using the Azure AI Evaluation SDK\n",
                "### Azure OpenAI + Azure AI Evaluation SDK\n",
                "### Name: (Your Name)\n",
                "<span>© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1"
            },
            "source": [
                "This notebook serves as a case study for evaluating Large Language Models (LLMs) using the Azure AI Evaluation SDK. We will cover the following metrics:\n",
                "- **Coherence**: Measures the logical flow and clarity of the response.\n",
                "- **Fluency**: Assesses the grammatical correctness and readability of the text.\n",
                "- **Groundedness**: Evaluates the accuracy of the information provided in the response.\n",
                "- **Relevance**: Determines how well the response addresses the query.\n",
                "- **Retrieval**: Measures the effectiveness of the model in retrieving relevant information."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Explanation of the Assignment Seccion\n",
                "\n",
                "The assignment section focuses on evaluating the groundedness of LLM responses using the Azure AI Evaluation SDK. Specifically, it challenges you to:\n",
                "\n",
                "- Identify factual inaccuracies by modifying an example response.\n",
                "- Simulate scenarios with subtle or multiple errors in a response.\n",
                "\n",
                "\n",
                "This exercise emphasizes the importance of precise factual information in model responses and demonstrates how nuanced errors can impact evaluation metrics. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install azure-ai-evaluation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Import the libraries "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Could not import AIAgentConverter. Please install the dependency with `pip install azure-ai-projects`.\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "import json\n",
                "import os\n",
                "from azure.ai.evaluation import (\n",
                "    CoherenceEvaluator,\n",
                "    FluencyEvaluator,\n",
                "    GroundednessEvaluator,\n",
                "    RelevanceEvaluator,\n",
                "    RetrievalEvaluator,AzureOpenAIModelConfiguration\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set the environment variable for the OpenAI API key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model_config = AzureOpenAIModelConfiguration(\n",
                "    azure_endpoint=\"replace with your endpoint\",# https://azure-openai-<your-resource-name>.openai.azure.com/\n",
                "    api_key=\"replace with your key\",# key from the Azure OpenAI resource\n",
                "    azure_deployment=\"gpt-4.1-mini\",\n",
                "    api_version=\"2023-05-15\"  \n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4"
            },
            "source": [
                "## Coherence Evaluation\n",
                "Coherence refers to the logical flow and clarity of the response. Let's evaluate a sample response for coherence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "5",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Coherence Score: {'coherence': 4.0, 'gpt_coherence': 4.0, 'coherence_reason': 'The response directly and clearly answers the question with a complete sentence, showing logical and orderly presentation of the idea. It is easy to follow and fully coherent.', 'coherence_result': 'pass', 'coherence_threshold': 3}\n"
                    ]
                }
            ],
            "source": [
                "coherence_eval = CoherenceEvaluator(model_config)\n",
                "\n",
                "query_response = dict(\n",
                "    query='What is the capital of France?',\n",
                "    response='The capital of France is Paris.'\n",
                ")\n",
                "\n",
                "coherence_score = coherence_eval(**query_response)\n",
                "print('Coherence Score:', coherence_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6"
            },
            "source": [
                "## Fluency Evaluation\n",
                "Fluency assesses the grammatical correctness and readability of the text. Let's evaluate a sample response for fluency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "7",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fluency Score: {'fluency': 3.0, 'gpt_fluency': 3.0, 'fluency_reason': 'The response is clear and grammatically correct but simple and lacks complexity or varied vocabulary, fitting the definition of competent fluency.', 'fluency_result': 'pass', 'fluency_threshold': 3}\n"
                    ]
                }
            ],
            "source": [
                "fluency_eval = FluencyEvaluator(model_config)\n",
                "\n",
                "query_response = dict(\n",
                "    query='How do you make a cake?',\n",
                "    response='To make a cake, you need flour, sugar, and eggs.'\n",
                ")\n",
                "\n",
                "fluency_score = fluency_eval(**query_response)\n",
                "print('Fluency Score:', fluency_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8"
            },
            "source": [
                "## Groundedness Evaluation\n",
                "Groundedness evaluates the accuracy of the information provided in the response. Let's evaluate a sample response for groundedness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Groundedness Score: 3.0\n",
                        "Explanation: The response attempts to answer the query by listing planets but incorrectly includes Pluto as a planet and omits Neptune, which is inaccurate based on the context. Hence, it contains incorrect information and is not fully grounded.\n"
                    ]
                }
            ],
            "source": [
                "# Create a groundedness evaluator\n",
                "groundedness_eval = GroundednessEvaluator(model_config)\n",
                "\n",
                "# Example with response that contains both correct and incorrect information\n",
                "query = \"Tell me about the solar system\"\n",
                "response = \"The solar system has 8 planets. Mercury is closest to the sun, followed by Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Pluto.\"\n",
                "context = \"The solar system has 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Pluto was reclassified as a dwarf planet in 2006.\"\n",
                "\n",
                "# Evaluate groundedness with context\n",
                "result = groundedness_eval(query=query, response=response, context=context)\n",
                "print(\"Groundedness Score:\", result['groundedness'])\n",
                "print(\"Explanation:\", result['groundedness_reason'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "10"
            },
            "source": [
                "## Relevance Evaluation\n",
                "Relevance determines how well the response addresses the query. Let's evaluate a sample response for relevance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "11",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Relevance Score: {'relevance': 4.0, 'gpt_relevance': 4.0, 'relevance_reason': 'The response correctly and completely answers the question by identifying the cheetah as the fastest land animal, fulfilling the requirement for a complete and accurate answer.', 'relevance_result': 'pass', 'relevance_threshold': 3}\n"
                    ]
                }
            ],
            "source": [
                "relevance_eval = RelevanceEvaluator(model_config)\n",
                "\n",
                "query_response = dict(\n",
                "    query='What is the fastest land animal?',\n",
                "    response='The cheetah is the fastest land animal.'\n",
                ")\n",
                "\n",
                "relevance_score = relevance_eval(**query_response)\n",
                "print('Relevance Score:', relevance_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "12"
            },
            "source": [
                "## Retrieval Evaluation\n",
                "Retrieval measures the effectiveness of the model in retrieving relevant information. Let's evaluate a sample response for retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieval score: {'retrieval': 5.0, 'gpt_retrieval': 5.0, 'retrieval_reason': 'The context directly and succinctly answers the query with the most relevant information at the top, making it highly relevant and well ranked without any bias.', 'retrieval_result': 'pass', 'retrieval_threshold': 3}\n"
                    ]
                }
            ],
            "source": [
                "# Use the existing model_config and RetrievalEvaluator already imported\n",
                "\n",
                "retrieval_eval = RetrievalEvaluator(model_config)\n",
                "\n",
                "query = \"What is the largest ocean on Earth?\"\n",
                "response = \"The Pacific Ocean is the largest ocean on Earth.\"\n",
                "context = \"The Pacific Ocean covers about 165 million km², making it the largest.\"\n",
                "\n",
                "score = retrieval_eval(query=query, response=response, context=context)\n",
                "print(\"Retrieval score:\", score)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Assignment\n",
                "\n",
                "### Groundedness Evaluation: \n",
                "In the groundedness example, the response incorrectly lists Pluto as a planet. Modify the example to test a different type of factual error and explain how the groundedness score might change when the error is more subtle or when multiple errors are present in a longer response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "# Create a groundedness evaluator\n",
                "groundedness_eval = GroundednessEvaluator(model_config)\n",
                "\n",
                "# Example with a more subtle factual error\n",
                "query = \"When did the first moon landing occur?\"\n",
                "response = \"The first moon landing was achieved by Neil Armstrong and Buzz Aldrin on July 21, 1969, when Apollo 11 landed on the lunar surface.\"\n",
                "context = \"NASA's Apollo 11 mission successfully landed the first humans on the Moon on July 20, 1969. Neil Armstrong and Buzz Aldrin stepped onto the lunar surface while Michael Collins orbited above.\"\n",
                "\n",
                "# Evaluate groundedness with context\n",
                "result = groundedness_eval(query=query, response=response, context=context)\n",
                "print(\"Groundedness Score:\", result['groundedness'])\n",
                "print(\"Explanation:\", result['groundedness_reason'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "14"
            },
            "source": [
                "## Conclusion\n",
                "In this case study, we have evaluated LLM responses using the Azure AI Evaluation SDK across various metrics: Coherence, Fluency, Groundedness, Relevance, and Retrieval. You can use these metrics to assess and improve the performance of your LLM applications."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
